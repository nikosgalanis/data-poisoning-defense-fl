{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted poisoning attack on MNIST dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 60\n",
    "n_test_clients = 60\n",
    "\n",
    "n_train_dataset_epochs = 6\n",
    "n_test_dataset_epochs = 3\n",
    "n_train_epochs = 30\n",
    "batch_size_train = 20\n",
    "batch_size_test = 4\n",
    "\n",
    "client_learning_rate = 0.02\n",
    "server_learning_rate = 1.5\n",
    "\n",
    "\n",
    "hidden_units = 256\n",
    "dropout = 0.1\n",
    "\n",
    "mal_users_percentage = 0.2\n",
    "# todo: could also be a list of values\n",
    "target_value = 3\n",
    "poisoned_value = 8\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f0000_14', 'f0001_41', 'f0005_26', 'f0006_12', 'f0008_45', 'f0011_13', 'f0014_19', 'f0016_39', 'f0017_07', 'f0022_10']\n",
      "['f0000_14', 'f0001_41', 'f0005_26', 'f0006_12', 'f0008_45', 'f0011_13', 'f0014_19', 'f0016_39', 'f0017_07', 'f0022_10']\n",
      "93\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
    "    emnist_train.client_ids[45])\n",
    "\n",
    "test_dataset = emnist_test.create_tf_dataset_for_client(\n",
    "    emnist_train.client_ids[45])\n",
    "\n",
    "print(emnist_train.client_ids[0:10])\n",
    "print(emnist_test.client_ids[0:10])\n",
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "\n",
    "for item in example_dataset:\n",
    "    cnt_train += 1 \n",
    "\n",
    "for item in test_dataset:\n",
    "    cnt_test += 1\n",
    "\n",
    "\n",
    "print(cnt_train)\n",
    "print(cnt_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and organizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer = 100\n",
    "# todo change??\n",
    "prefetch_buffer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_format(element):\n",
    "    # flatten the images\n",
    "    return collections.OrderedDict(\n",
    "        x = tf.reshape(element['pixels'], [-1, 28, 28]),\n",
    "        y = tf.reshape(element['label'], [-1, 1]))\n",
    "\n",
    "def preprocess(dataset, train):\n",
    "    if train == True:\n",
    "        dataset = dataset.repeat(n_train_dataset_epochs)\n",
    "    else:\n",
    "        dataset = dataset.repeat(n_test_dataset_epochs)\n",
    "        \n",
    "    dataset = dataset.shuffle(shuffle_buffer, seed = 1)\n",
    "    if train == True:\n",
    "        dataset = dataset.batch(batch_size_train)\n",
    "    else: \n",
    "        dataset = dataset.batch(batch_size_test)\n",
    "        \n",
    "    dataset = dataset.map(batch_format)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_dataset(dataset, target_honest, target_mal):\n",
    "    # parse the dataset\n",
    "    for batch in dataset:\n",
    "        # get the labels of each batch and convert to numpy array\n",
    "        labels = batch['y'].numpy()\n",
    "        # itterate through each label\n",
    "        for i,y in enumerate(labels):\n",
    "            # if we find the target label\t\n",
    "            if y == target_honest:\n",
    "                labels[i] = target_mal\n",
    "        batch['y'] = tf.convert_to_tensor(labels, dtype = tf.float32)\n",
    "        # print(batch['y'])\n",
    "    # return the malicious dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "## testing the preprocessing function\n",
    "\n",
    "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
    "    emnist_train.client_ids[0])\n",
    "     \n",
    "preprocessed_example_dataset = preprocess(example_dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_federated_data(client_data, client_ids, train, mal_users_percentage=0):\n",
    "    target_honest = 3\n",
    "    target_mal = 8\n",
    "    fed_data = []\n",
    "    for id in client_ids:\n",
    "        preprocessed_dataset = preprocess(client_data.create_tf_dataset_for_client(id), train)\n",
    "        prob = random.random()\n",
    "        # mal% of the users are malicious\n",
    "        if prob < mal_users_percentage:  \n",
    "            preprocessed_dataset = poison_dataset(preprocessed_dataset, target_honest, target_mal)              \n",
    "\n",
    "        fed_data.append(preprocessed_dataset)\n",
    "\n",
    "    return fed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(emnist_test.client_ids[0])\n",
    "data = make_federated_data(emnist_test, [emnist_train.client_ids[0]], train=False, mal_users_percentage=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "      return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Reshape(input_shape=(28,28,1), target_shape=(28,28,1)),\n",
    "      tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dropout(dropout),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_model():\n",
    "    keras_model = create_model()\n",
    "    return tff.learning.models.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec = preprocessed_example_dataset.element_spec,\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    mnist_model,\n",
    "    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = client_learning_rate),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate = server_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = training_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# build the process to have the model's architecture\n",
    "evaluation_process = tff.learning.algorithms.build_fed_eval(mnist_model)\n",
    "\n",
    "# initialize the state of the evaluation\n",
    "evaluation_state = evaluation_process.initialize()\n",
    "sample_test_clients = emnist_test.client_ids[0:n_test_clients]\n",
    "\n",
    "\n",
    "# step = 16\n",
    "# sample_test_clients = [emnist_train.client_ids[x] for x in range (0, n_clients * step, step)]\n",
    "\n",
    "# test the model with the test data\n",
    "# question: selection of clients during training??\n",
    "federated_test_data = make_federated_data(emnist_test, sample_test_clients, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training accuracy: 0.127, Training loss: 2.300\n",
      "Testing accuracy: 0.164, Testing loss: 2.279\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "Training accuracy: 0.210, Training loss: 2.252\n",
      "Testing accuracy: 0.421, Testing loss: 2.240\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "Training accuracy: 0.342, Training loss: 2.207\n",
      "Testing accuracy: 0.551, Testing loss: 2.187\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Training accuracy: 0.496, Training loss: 2.142\n",
      "Testing accuracy: 0.604, Testing loss: 2.111\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "Training accuracy: 0.576, Training loss: 2.056\n",
      "Testing accuracy: 0.636, Testing loss: 2.007\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Training accuracy: 0.621, Training loss: 1.927\n",
      "Testing accuracy: 0.661, Testing loss: 1.868\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "Training accuracy: 0.695, Training loss: 1.780\n",
      "Testing accuracy: 0.711, Testing loss: 1.692\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Training accuracy: 0.770, Training loss: 1.555\n",
      "Testing accuracy: 0.712, Testing loss: 1.494\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "Training accuracy: 0.798, Training loss: 1.354\n",
      "Testing accuracy: 0.731, Testing loss: 1.295\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Training accuracy: 0.808, Training loss: 1.188\n",
      "Testing accuracy: 0.754, Testing loss: 1.128\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Training accuracy: 0.830, Training loss: 1.020\n",
      "Testing accuracy: 0.763, Testing loss: 0.988\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Training accuracy: 0.859, Training loss: 0.829\n",
      "Testing accuracy: 0.778, Testing loss: 0.882\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Training accuracy: 0.863, Training loss: 0.758\n",
      "Testing accuracy: 0.773, Testing loss: 0.808\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Training accuracy: 0.865, Training loss: 0.672\n",
      "Testing accuracy: 0.784, Testing loss: 0.751\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Training accuracy: 0.879, Training loss: 0.597\n",
      "Testing accuracy: 0.785, Testing loss: 0.699\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Training accuracy: 0.887, Training loss: 0.541\n",
      "Testing accuracy: 0.784, Testing loss: 0.674\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Training accuracy: 0.883, Training loss: 0.542\n",
      "Testing accuracy: 0.801, Testing loss: 0.630\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Training accuracy: 0.872, Training loss: 0.536\n",
      "Testing accuracy: 0.797, Testing loss: 0.619\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Training accuracy: 0.886, Training loss: 0.496\n",
      "Testing accuracy: 0.798, Testing loss: 0.592\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Training accuracy: 0.905, Training loss: 0.421\n",
      "Testing accuracy: 0.807, Testing loss: 0.563\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Training accuracy: 0.899, Training loss: 0.428\n",
      "Testing accuracy: 0.820, Testing loss: 0.551\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "Training accuracy: 0.913, Training loss: 0.370\n",
      "Testing accuracy: 0.820, Testing loss: 0.527\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "Training accuracy: 0.897, Training loss: 0.410\n",
      "Testing accuracy: 0.826, Testing loss: 0.520\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "Training accuracy: 0.908, Training loss: 0.392\n",
      "Testing accuracy: 0.829, Testing loss: 0.499\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "Training accuracy: 0.913, Training loss: 0.353\n",
      "Testing accuracy: 0.833, Testing loss: 0.488\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "Training accuracy: 0.917, Training loss: 0.342\n",
      "Testing accuracy: 0.839, Testing loss: 0.477\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "Training accuracy: 0.927, Training loss: 0.313\n",
      "Testing accuracy: 0.835, Testing loss: 0.472\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "Training accuracy: 0.930, Training loss: 0.297\n",
      "Testing accuracy: 0.845, Testing loss: 0.457\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "Training accuracy: 0.921, Training loss: 0.307\n",
      "Testing accuracy: 0.842, Testing loss: 0.445\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "training_loss = []\n",
    "training_acc = []\n",
    "eval_loss = []\n",
    "eval_acc = []\n",
    "\n",
    "clients = emnist_train.client_ids[0:n_clients]\n",
    "step = 16\n",
    "clients = [emnist_train.client_ids[x] for x in range (0, n_clients * step, step)]\n",
    "\n",
    "#note: slow to converge with random clients, makes sense\n",
    "\n",
    "for epoch in range(1, n_train_epochs):\n",
    "\n",
    "    print('Epoch: {:2d}'.format(epoch))\n",
    "    # client selection, random, chosen from the first 100 clients\n",
    "    clients = random.sample(emnist_train.client_ids, n_clients)\n",
    "\n",
    "    federated_train_data = make_federated_data(emnist_train, clients, train=True)\n",
    "    federated_test_data = make_federated_data(emnist_test, sample_test_clients, train=False)\n",
    "    \n",
    "    \n",
    "    # run a next on the training process to train the model\n",
    "    result = training_process.next(train_state, federated_train_data)\n",
    "    # update the model's state and get access to the metrics\n",
    "    train_state = result.state\n",
    "    train_metrics = result.metrics\n",
    "    # print the training metrics\n",
    "    training_acc.append(train_metrics['client_work']['train']['sparse_categorical_accuracy'])\n",
    "    training_loss.append(train_metrics['client_work']['train']['loss'])\n",
    "\n",
    "    print('Training accuracy: {:.3f}, Training loss: {:.3f}'.format(training_acc[-1], training_loss[-1]))\n",
    "\n",
    "    # evaluate the model with test data\n",
    "\n",
    "    # get weights from the trainged model\n",
    "    model_weights = training_process.get_model_weights(train_state)\n",
    "    # update the evaluation state with them\n",
    "    evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
    "    # run a next() to evaluate the model\n",
    "    evaluation_output = evaluation_process.next(evaluation_state, federated_test_data)\n",
    "\n",
    "    # get access to the evaluation metrics\n",
    "    eval_metrics = evaluation_output.metrics['client_work']['eval']['total_rounds_metrics']\n",
    "\n",
    "    eval_acc.append(eval_metrics['sparse_categorical_accuracy'])\n",
    "    eval_loss.append(eval_metrics['loss'])\n",
    "    # print the training metrics\n",
    "    print('Testing accuracy: {:.3f}, Testing loss: {:.3f}\\n\\n'.format(eval_acc[-1], eval_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningAlgorithmState(global_model_weights=ModelWeights(trainable=[array([[[[ 1.40045181e-01,  2.50327855e-01, -7.52348900e-02,\n",
      "           3.43430117e-02,  6.40342608e-02, -4.92724478e-02,\n",
      "           4.24161017e-01, -7.60460598e-03,  1.05704226e-01,\n",
      "           3.49419951e-01,  9.68406349e-02,  1.53103480e-02,\n",
      "           2.81466246e-02, -3.42142075e-01,  3.04258227e-01,\n",
      "          -3.59155506e-01, -2.14422613e-01,  3.75122875e-01,\n",
      "          -4.79744703e-01,  3.94661240e-02,  5.00482041e-03,\n",
      "           2.27420658e-01,  3.02444518e-01, -6.21368647e-01,\n",
      "          -9.77644771e-02,  3.77750456e-01,  1.17097057e-01,\n",
      "          -2.54666880e-02,  4.58100349e-01, -1.69613868e-01,\n",
      "          -9.31188688e-02,  1.35494262e-01]],\n",
      "\n",
      "        [[-2.33093992e-01,  2.57363975e-01,  6.58216886e-05,\n",
      "          -4.94578987e-01,  3.52748781e-01, -7.24928752e-02,\n",
      "           8.47739354e-02,  1.08471755e-02,  9.80660468e-02,\n",
      "           6.19769357e-02,  1.51732504e-01, -7.17567354e-02,\n",
      "          -3.15265000e-01, -4.42403972e-01, -1.09390780e-01,\n",
      "           4.10064518e-01, -4.37183887e-01,  9.55412686e-02,\n",
      "          -5.62462926e-01,  6.21076450e-02,  1.95739530e-02,\n",
      "           1.46401629e-01,  3.60355854e-01, -3.39455515e-01,\n",
      "          -9.83518139e-02, -1.58139944e-01, -7.75451735e-02,\n",
      "           4.14499789e-01,  4.84542131e-01, -6.70728266e-01,\n",
      "          -6.40857639e-03, -1.88761055e-01]],\n",
      "\n",
      "        [[-3.06944400e-01,  1.58690065e-01,  2.07966253e-01,\n",
      "           2.02263609e-01,  4.75499839e-01,  2.04283688e-02,\n",
      "          -2.56276011e-01,  8.47649798e-02,  2.26048395e-01,\n",
      "          -2.70122796e-01,  1.91804752e-01, -7.83166382e-03,\n",
      "          -1.43416626e-02,  2.13106781e-01, -3.56403112e-01,\n",
      "           5.93903422e-01,  4.97413576e-01, -3.16198796e-01,\n",
      "          -8.84650722e-02, -7.98199102e-02, -1.59374282e-01,\n",
      "          -2.46460870e-01, -3.31170671e-02, -4.77230668e-01,\n",
      "          -3.08526427e-01, -3.13461512e-01, -3.14480007e-01,\n",
      "           3.43042314e-01,  4.50697154e-01, -7.56316543e-01,\n",
      "          -1.17179908e-01, -4.36533570e-01]]],\n",
      "\n",
      "\n",
      "       [[[-1.46115109e-01,  1.42790958e-01,  9.83772352e-02,\n",
      "          -4.85118687e-01, -4.37220305e-01, -7.40247145e-02,\n",
      "           2.77841330e-01,  4.41591926e-02, -2.37089023e-01,\n",
      "           2.89975613e-01,  1.01183563e-01, -4.86240201e-02,\n",
      "           3.88567746e-02,  4.26956415e-02,  2.39443541e-01,\n",
      "          -5.97827792e-01, -4.68105257e-01,  7.30861574e-02,\n",
      "          -9.07109752e-02,  3.30673829e-02, -1.46927550e-01,\n",
      "           2.29565531e-01,  3.86491805e-01, -1.06215030e-01,\n",
      "          -2.09890217e-01,  8.64769891e-02,  2.43127704e-01,\n",
      "          -1.18104562e-01,  3.14784609e-03, -1.69043332e-01,\n",
      "           4.36049812e-02,  2.36241981e-01]],\n",
      "\n",
      "        [[-1.55533880e-01,  7.74071813e-02,  1.22065172e-01,\n",
      "          -3.31348389e-01, -1.34879515e-01, -1.16320081e-01,\n",
      "          -1.88159302e-01, -5.53815924e-02, -4.48125213e-01,\n",
      "          -1.86167285e-01,  1.85430497e-01, -1.14110246e-01,\n",
      "          -2.02896148e-01, -4.07154977e-01,  1.57373905e-01,\n",
      "          -2.58681029e-01, -3.28686833e-01, -1.02287963e-01,\n",
      "          -9.87520814e-02,  8.23679045e-02, -2.68504679e-01,\n",
      "          -9.72801596e-02,  2.96616524e-01,  1.45146370e-01,\n",
      "          -3.36181760e-01, -2.65245557e-01, -2.45892312e-02,\n",
      "          -5.60640618e-02, -1.92919225e-01,  1.78179786e-01,\n",
      "           4.13867123e-02, -1.28358468e-01]],\n",
      "\n",
      "        [[ 8.29687417e-02, -1.17964283e-01,  1.99393183e-01,\n",
      "           4.89115596e-01,  2.48824209e-01, -1.28717035e-01,\n",
      "          -3.80609512e-01, -4.91952598e-02, -2.81008363e-01,\n",
      "          -3.55265796e-01,  5.74599020e-02, -3.24263461e-02,\n",
      "           1.81515574e-01, -1.05038486e-01, -5.10343254e-01,\n",
      "           3.67659718e-01,  5.94407320e-01, -2.89034873e-01,\n",
      "          -2.53895938e-01,  1.95849147e-02, -5.01824319e-01,\n",
      "          -1.75729781e-01, -5.21960497e-01,  2.12693617e-01,\n",
      "          -3.94300699e-01, -3.30167353e-01,  5.19276038e-02,\n",
      "           3.20825487e-01, -2.07673430e-01, -1.34274885e-01,\n",
      "          -7.01358616e-02, -3.07226758e-02]]],\n",
      "\n",
      "\n",
      "       [[[-3.80109176e-02, -5.34009226e-02,  6.11314140e-02,\n",
      "          -3.31774235e-01, -4.99314517e-01,  1.17754936e-01,\n",
      "           2.24897906e-01, -9.34936926e-02, -1.29832998e-01,\n",
      "           2.45861158e-01,  1.31431490e-01,  3.20992665e-03,\n",
      "          -5.28459549e-02, -7.99854547e-02,  4.14885104e-01,\n",
      "          -3.12439591e-01, -5.27419806e-01, -3.74630123e-01,\n",
      "           2.19996348e-01,  6.20587803e-02,  4.06208336e-01,\n",
      "          -2.64557987e-01,  1.08534239e-01,  4.45328653e-01,\n",
      "           3.28642190e-01,  4.41861451e-02,  1.41097561e-01,\n",
      "          -3.78280461e-01, -5.79081655e-01,  5.56091666e-01,\n",
      "          -1.68620050e-02,  3.35664809e-01]],\n",
      "\n",
      "        [[ 2.04239413e-01, -3.24962914e-01,  8.33925828e-02,\n",
      "           2.10035235e-01, -2.16683239e-01, -8.21057856e-02,\n",
      "          -4.24267143e-01, -1.00265726e-01,  1.08552597e-01,\n",
      "          -3.19566220e-01,  8.34165290e-02,  9.11675394e-03,\n",
      "          -1.24389678e-01,  2.39652455e-01,  3.85078900e-02,\n",
      "          -3.66198391e-01, -7.88528547e-02, -2.92546395e-02,\n",
      "           5.21098256e-01,  9.42176506e-02,  4.07925248e-01,\n",
      "          -8.01949874e-02, -2.29616672e-01,  2.96800852e-01,\n",
      "           4.25189435e-01,  2.36905083e-01, -1.56447276e-01,\n",
      "          -5.26493549e-01, -3.82771552e-01,  4.84782845e-01,\n",
      "          -1.16456628e-01, -8.21006522e-02]],\n",
      "\n",
      "        [[ 2.07410678e-01, -4.51390743e-01,  1.75085068e-01,\n",
      "           3.16139698e-01, -2.40315706e-01, -7.48465359e-02,\n",
      "           3.56227309e-02,  1.09343473e-02,  2.02867165e-01,\n",
      "          -5.83592057e-02,  1.06849082e-01, -5.99012114e-02,\n",
      "           2.19110698e-01,  4.26276028e-01, -3.90566289e-01,\n",
      "          -2.39560362e-02,  5.01921892e-01,  2.78618783e-01,\n",
      "           4.16707188e-01,  4.84790802e-02, -5.36088236e-02,\n",
      "           6.43357113e-02, -7.77020276e-01,  3.45130682e-01,\n",
      "           3.72548282e-01,  1.90230180e-02,  1.62343860e-01,\n",
      "          -2.98987389e-01, -3.73114228e-01,  3.48651230e-01,\n",
      "          -1.25681937e-01, -7.32856840e-02]]]], dtype=float32), array([ 2.33414888e-01,  7.81639963e-02, -1.92215830e-01,  3.62667292e-01,\n",
      "        3.82570714e-01,  6.61112601e-03,  1.89632833e-01,  1.44357095e-03,\n",
      "        3.53342384e-01,  2.29896218e-01, -2.34933376e-01,  1.21559156e-03,\n",
      "        2.30950385e-01,  4.46976006e-01,  2.08015606e-01,  5.15900135e-01,\n",
      "        4.39528465e-01,  2.80276686e-01,  4.18582380e-01, -6.95084706e-02,\n",
      "        2.76593983e-01,  1.88196361e-01,  1.19111784e-01,  1.16121762e-01,\n",
      "        3.13805312e-01,  3.01081091e-01, -3.62482704e-02,  3.13133061e-01,\n",
      "        3.38672251e-01,  3.27084243e-01,  2.38858367e-04,  2.31697723e-01],\n",
      "      dtype=float32), array([[ 2.5328951e-02, -3.0788125e-02,  2.7468950e-02, ...,\n",
      "        -3.0108463e-02,  2.2073746e-02, -3.0041385e-02],\n",
      "       [ 1.2489554e-02, -8.4252972e-03, -1.4987510e-02, ...,\n",
      "         1.1909972e-02,  1.2699413e-03, -1.7592072e-02],\n",
      "       [-2.9798804e-02,  4.4372141e-02,  1.3977397e-02, ...,\n",
      "        -2.3781905e-02, -1.8336099e-02,  1.9029390e-02],\n",
      "       ...,\n",
      "       [-2.8806889e-02, -2.5309423e-02,  3.1402849e-03, ...,\n",
      "        -1.8087214e-02, -3.0580956e-02, -2.9839234e-02],\n",
      "       [-2.1079183e-04,  2.6054990e-02, -3.9234068e-03, ...,\n",
      "         5.5457763e-03, -1.5420126e-02,  6.3095391e-03],\n",
      "       [-9.6871525e-05,  2.9203467e-02,  1.2414388e-02, ...,\n",
      "         2.5793111e-02,  2.6178325e-02,  2.5691334e-02]], dtype=float32), array([-0.00512208,  0.04818045, -0.00565729, -0.00480317, -0.00945584,\n",
      "        0.004254  , -0.00206741,  0.02497039, -0.03322056, -0.01707847],\n",
      "      dtype=float32)], non_trainable=[]), distributor=(), client_work=((), OrderedDict([('sparse_categorical_accuracy', [1728.0, 2052.0]), ('loss', [913.26184, 2052.0]), ('num_examples', [2052]), ('num_batches', [532])])), aggregator=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), finalizer=())\n",
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('eval', OrderedDict([('current_round_metrics', OrderedDict([('sparse_categorical_accuracy', 0.84210527), ('loss', 0.4450594), ('num_examples', 2052), ('num_batches', 532)])), ('total_rounds_metrics', OrderedDict([('sparse_categorical_accuracy', 0.84210527), ('loss', 0.4450594), ('num_examples', 2052), ('num_batches', 532)]))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch in evaluation_output:\n",
    "    print(batch)\n",
    "    # pred = np.argmax(batch)\n",
    "\n",
    "#     print(batch['y'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train\n",
    "# clients = emnist_train.client_ids[0:n_clients]\n",
    "# print(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_output.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(1, n_train_epochs)]\n",
    "plt.plot(epochs, training_acc)\n",
    "plt.plot(epochs, eval_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(1, n_train_epochs)]\n",
    "plt.plot(epochs, training_loss)\n",
    "plt.plot(epochs, eval_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 0% mal clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a169994204ac44a2b22833e736d565f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1702602/2774952245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model with \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"% mal clients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# client selection, random, chosen from the first 100 clients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
    "\n",
    "accs = []\n",
    "losses = []\n",
    "\n",
    "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
    "    emnist_train.client_ids[0])\n",
    "\n",
    "preprocessed_example_dataset = preprocess(example_dataset, True)\n",
    " \n",
    "     \n",
    "# build the process to have the model's architecture\n",
    "evaluation_process = tff.learning.algorithms.build_fed_eval(mnist_model)\n",
    "\n",
    "# initialize the state of the evaluation\n",
    "sample_test_clients = emnist_test.client_ids[0:n_test_clients]\n",
    "\n",
    "federated_test_data = make_federated_data(emnist_test, sample_test_clients, train=False)\n",
    "\n",
    "\n",
    "n_train_epochs = 20\n",
    "\n",
    "for i in [0, 5, 6, 7]:\n",
    "    evaluation_state = evaluation_process.initialize()\n",
    "\n",
    "    eval_acc = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    mal_users_percentage = i / 10\n",
    "    \n",
    "    emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
    "\n",
    "    training_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
    "        mnist_model,\n",
    "        client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = client_learning_rate),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate = server_learning_rate))\n",
    "\n",
    "    train_state = training_process.initialize()\n",
    "    print(\"Model with \" + str(i) + \"% mal clients\")\n",
    "    for epoch in tqdm(range(0, n_train_epochs)):\n",
    "        \n",
    "        # client selection, random, chosen from the first 100 clients\n",
    "        # clients = random.sample(emnist_train.client_ids, n_clients)\n",
    "        clients = emnist_train.client_ids[0:n_clients]\n",
    "\n",
    "        federated_train_data = make_federated_data(emnist_train, clients, train=True, mal_users_percentage=mal_users_percentage)      \n",
    "        \n",
    "        # run a next on the training process to train the model\n",
    "        result = training_process.next(train_state, federated_train_data)\n",
    "        # update the model's state and get access to the metrics\n",
    "        train_state = result.state\n",
    "                \n",
    "        # get weights from the trainged model\n",
    "        model_weights = training_process.get_model_weights(train_state)\n",
    "        # update the evaluation state with them\n",
    "        evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
    "        # run a next() to evaluate the model\n",
    "        evaluation_output = evaluation_process.next(evaluation_state, federated_test_data)\n",
    "\n",
    "        # get access to the evaluation metrics\n",
    "        eval_metrics = evaluation_output.metrics['client_work']['eval']['total_rounds_metrics']\n",
    "\n",
    "        eval_acc.append(eval_metrics['sparse_categorical_accuracy'])\n",
    "        eval_loss.append(eval_metrics['loss'])        \n",
    "\n",
    "    accs.append(eval_acc)\n",
    "    losses.append(eval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x for x in range(0, n_train_epochs)]\n",
    "for cnt, model_acc in enumerate(accs):\n",
    "\n",
    "    plt.plot(epochs, model_acc, label=str(cnt * 10) + \"% of mal users\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
