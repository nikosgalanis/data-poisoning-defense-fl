{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks on Federated Learning Models\n",
    "Summarised from “Federated Learning Attacks and Defenses: A Survey.”, Chen et al, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning Attacks\n",
    "\n",
    "### Data Poisoning\n",
    " - An attacker tampering with or adding data to the training set maliciously, which eventually leads to the destruction or hijacking of the model.\n",
    " - Implemented by the owner of the private data.\n",
    "\n",
    "### Model Poisoning\n",
    " - The attacker changes the parameters of the target model directly, causing errors in the global model or leaving a backdoor.\n",
    " - More efficient than data poisoning\n",
    " - Perturbing the weights of convolutional neural networks in a targeted manner can be used to insert backdoors\n",
    " - By using available bit-flipping techniques, the target model can be converted into the Trojan infection model ["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Attacks\n",
    " - Although the data transmitted from the client to the server is not the original data, there is still a risk of leakage\n",
    " - Inference attacks indicate that attackers can use the eavesdropped information to infer useful information to a certain extent, which obviously destroys the confidentiality of the model.\n",
    " - How to: train a model with similar functions on a specified sample and then judge whether the sample is used for the target model’s training.\n",
    " - There have been studies to prove that this attack can infer the accent information of the training set in FL of speech recognition.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Attacks\n",
    " - Unlike inference attacks that cannot reveal raw data, reconstruction attacks can obtain the original information of the training dataset by collecting some information such as predicted confidence values, model parameters, and gradients.\n",
    " - Confidentiality attack\n",
    " - First approach: Shows that when the adversary has white-box access rights, the adversary can use the linear model to estimate the attribute information of the original data.\n",
    " - **Generative Adversarial Networks (GAN)**: Points out that attackers can obtain samples of other participants, and this process only requires black-box access.\n",
    " - **Deep Leakage from Gradients (DLG)** shows that the attacker could recover the original data by analyzing the gradient information\n",
    " - **Improved Deep Leakage from Gradients (DLG)**: Improves the accuracy of data recovery\n",
    " - **Inverting gradients based on DLG** were proposed and it broadened the attack scenario to include the actual industrialized scenario rather than being limited to the strong assumption of low-resolution recovery and a shallow network.\n",
    " - **Generative Regression Neural Network (GRNN)** based on GAN was proposed to restore the original training data without the need for additional information. It indicated that GRNN has stronger robustness and higher accuracy than previous methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Extraction Attack\n",
    " - When FL has finished training the model, the global model will serve outsiders in the form of an API. At this time, the user may query the relevant information of the target model through the API loop and finally achieve the effect of extracting the model.\n",
    " - Tram` er et al. first demonstrated that this attack will be effective when the attacker has the same distribution of data and model-related information as the model.\n",
    " - Another attack proposed can obtain hyperparameters located at the bottom layer of the model\n",
    " - Orekondy et al. proposed the Knockoff Net, through which attackers can steal based on the confidence value output by the API, and the stealing effect is positively correlated with the complexity of the target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evasion Attacks (seem interesting)\n",
    "\n",
    " - Evasion attack is a type of attack in which an attacker deceives the target machine learning system by constructing specific input samples without changing the target machine learning system.\n",
    " - Usually occurs during the prediction phase, when the model has finished training.\n",
    " - The effect of this kind of attack can be summarized as the model extrapolates the original answer “A” to be the wrong answer “B”.\n",
    "  - Evasion attack is an integrity attack due to the spoofing of the model.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byzantine Attacks\n",
    " - Byzantine attack is a type of attack in which an attacker hides among the participants of FL and arbitrarily uploads malicious data, aiming to destroy the global model\n",
    " - To deal with this attack, it is common to combine stochastic gradient descent (SGD) with different robust aggregation rules\n",
    " - Similar to this idea, it has been proven that by poisoning the local model, the global model has a large test error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defences\n",
    " - Privacy level and the security level.\n",
    " - Privacy refers to private information that a person does not want others to know and invade, focusing on sensitive personal information. The attack on FL is considered at the privacy level, when the attacker tries to infer private information about the participant. Privacy protection methods are used to defend against privacy attacks and to ensure that sensitive data is not leaked to others.\n",
    " - Security focuses on confidential data and information assets, not just personal information.\n",
    " - The security attack is a malicious action performed by hackers with specialized knowledge to compromise the confidentiality, integrity, and availability of data and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data anonymization\n",
    " - we can use anonymization techniques to hide or remove sensitive personal attributes, such as personally identifiable information (PII)\n",
    " - K-anonymity, L-diversity, and T-closeness are three common anonymization techniques.\n",
    " - This type of approach improves privacy by hiding or removing sensitive information, but may reduce the usability of the dataset. In addition, much anonymized data can be easily “de-anonymized”.\n",
    " - Data anonymization is often used in conjunction with other ways to protect privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Privacy (DP)\n",
    " - already known :)\n",
    " - DP defends against attacks during the training phase and the prediction phase of FL.\n",
    " - Those types of attacks that DP defends against are poisoning attack, inference attack, evasion attack, reconstruction attack, and model inversion\n",
    " - the utility of the models can be seriously affected if too much noise is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure multi-party computation (SMC)\n",
    " - Generic cryptographic primitive for solving privacy-preserving collaborative computation problems between a set of mutually distrusting participants\n",
    " - does not leak the input and output of the participant to other members participating in the computation.\n",
    " - able to defend against inference attacks, reconstruction attacks, model inversion, and leaks from malicious center servers\n",
    " - large computational overhead and high performance loss.\n",
    " - This may reduce the participants’ interest in cooperating, so SMC is not suitable for large-scale FL scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homomorphic encryption (HE)\n",
    " - It does not touch the original data. HE first encrypts the data, then processes it, and finally decrypts it.\n",
    "  - defends against attacks during the training phase and the prediction phase of FL\n",
    " - HE incurs a large computational overhead. Therefore, this approach is not suitable for FL scenarios with numerous participants and devices with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trusted execution environment (TEE)\n",
    " - Tamper-proof and trusted ecosystem for executing authenticated and verified code.\n",
    " - TEE establishes digital trust by protecting devices in FL, which effectively prevents attackers from attacking private information\n",
    " - Can defend against attacks such as MIA, PIA, mode inversion, and malicious server. \n",
    " - TEE has limited execution space, which prevents complex transaction logic from being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockchain\n",
    " - It is a distributed ledger technology that uses the blockchain to verify and store data, generates and updates data through a consensus mechanism, and involves an intelligent contract and an incentive mechanism\n",
    " - blockchain technology is decentralized, tamper-proof, unforgeable, auditable, and accountable\n",
    " - a preferred solution to the problem of implementing data security and data validation in a non-centralized FL scenario\n",
    " - Drawback that it can’t be used in FL situations with a lot of people and devices with limited computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Level\n",
    " - To ensure the security of the FL framework, we want to scan for all vulnerabilities as much as possible.\n",
    "\n",
    "Three main sources of vulnerabilities: \n",
    " - insecure communication channels\n",
    " - malicious clients\n",
    " - central parameter servers that are not robust or secure enough.\n",
    "\n",
    "Defences:\n",
    " - **Active**: Detect and mitigate the risk to the FL framework in advance, before it has an impact on the framework.\n",
    " - **Passive**: Remediate and mitigate the impact when an attack has already occurred.\n",
    "\n",
    "The security defense approach is closely related to the CIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidentiality\n",
    " - Inference attack, reconstruction attack, and model extraction attack\n",
    " - many defensive approaches have been proposed to ensure the confidentiality of data. \n",
    " - Ex: VerifyNet, a verifiable FL framework, can guarantee the confidentiality of the gradients uploaded by participants using the proposed double masking protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrity\n",
    " - Poisoning attack and Evasion attack are types of attacks that compromise integrity.\n",
    " - Purpose: ensure that once data is collected, it cannot be tampered with.\n",
    " - Methods to ensure data integrity are TEE and blockchain\n",
    " - Several methods to ensure integrity by screening malicious clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Availability\n",
    " - The attacks on availability in the FL framework are related to Byzantine attacks.\n",
    " - aggregation rule with resilience properties that can be used to defend against Byzantine attacks\n",
    " - Incentives in the FL framework is a good way to improve data availability by rewarding or penalizing participants based on the value of their contributions. (interesting)\n",
    " - This reduces the possibility of participants sending useless or harmful data, and also improves the usability of the training model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": 2
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}